\chapter{Аналитическая часть}

\section{Обучение с подкреплением}
Обучение с подкреплением (reinforcement learning, RL) представляет собой область машинного обучения, 
в которой агент учится действовать в среде с целью максимизации накопленной награды \cite{lib:rl}. 
Основными компонентами системы RL являются:
\begin{itemize}
    \item агент -- обучаемая система, принимающая решения;
    \item среда -- окружение, с которым взаимодействует агент;
    \item награда -- числовая величина, определяющая успех действия агента;
    \item состояние -- описание текущего положения среды;
    \item действие -- выбор агента, влияющий на среду.
\end{itemize}

Процесс обучения состоит из взаимодействия агента со средой, при котором агент наблюдает текущее состояние, 
выбирает действие, получает награду и новое состояние. 
Этот процесс можно представить как задачу оптимизации \cite{lib:rl}, где цель — максимизация ожидаемой совокупной награды, задаваемой следующим образом:
\begin{equation}
G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1},
\end{equation}
где $G_t$ — ожидаемая совокупная награда с момента времени $t$, $\gamma \in [0, 1]$ — коэффициент дисконтирования, $R_{t+k+1}$ — награда, полученная на шаге $t+k+1$.

Существует множество алгоритмов обучения с подкреплением, которые можно разделить на три основные группы: 
методы обучения политики, методы обучения ценности и гибридные методы. 

Политика (обычно обозначается $\pi$) определяет стратегию агента по выбору действий. 
Формально она представляется как отображение состояний в действия (или в распределения вероятностей по действиям).

Ценность (функции ценности) оценивают желательность пребывания в определенном состоянии или выполнения конкретного действия. 
В частности, функции ценности состояния оценивают ожидаемую кумулятивную награду, начиная с состояния s и следуя определенной политике.

\section{Proximal Policy Optimization (PPO)}
Proximal Policy Optimization (PPO) — это метод обучения политики, который улучшает стабильность и эффективность 
обучения за счет ограничения изменения политики между обновлениями \cite{lib:rlmethods}. 
Цель PPO -- оптимизировать стратегию, максимизируя ожидаемое вознаграждение, при этом
ограничивая изменения стратегии на каждом шаге обучения, чтобы обеспечить стабильность обучения. 

Основная идея PPO заключается в максимизации целевой функции, которая записывается следующим образом:
\begin{equation}
L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) A_t \right) \right],
\end{equation}
где 
$\mathbb{E}_t$ -- математическое ожидание по временным шагам $t$, 
$A_t$ -- оценка преимущества (advantage) в момент времени $t$ которая показывает, насколько действие $a_t$ в состоянии $s_t$ лучше или хуже среднего ожидаемого действия в этом состоянии,
$\epsilon$ -- гиперпараметр, ограничивающий изменение политики,
$\text{clip}(...)$ -- функция, обрезающая значение $r_t(\theta)$, чтобы отношение вероятностей не уходило слишком далеко от единицы,
$\min(...)$ -- функция минимума, ограничивающая обновление политики снизу, предотвращая слишком пессимистичные обновления в случаях, когда $A_t < 0$.

Отдельно стоит отметить функцию:
\begin{equation}
    r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
    \label{eq:rt}
\end{equation}
где
$r_t(\theta)$ -- показывает, насколько вероятнее стало действие при новой политике по сравнению со старой,
$\pi_\theta(a_t|s_t)$ -- вероятность совершить действие $a_t$ в состоянии $s_t$ в момент времени $t$ в соответствии с текущей политикой, определяемой параметрами $\theta$,
$\pi_{\theta_{old}}(a_t|s_t)$ -- то же самое но для старой политики, определяемой параметрами $\theta_{old}$.

\section{Soft Actor-Critic (SAC)}
Soft Actor-Critic (SAC) -- это алгоритм обучения с подкреплением, основанный на идее максимизации ожидаемой совокупной награды 
с одновременным увеличением энтропии политики \cite{lib:rlmethods}. 
Агент стремится не только получать высокие награды, но и сохранять высокую степень случайности в своих действиях, 
что способствует исследованию среды и предотвращает преждевременную сходимость к субоптимальным решениям.

Функция потерь записывается следующим образом:
\begin{equation}
L_{SAC}(\pi) = \mathbb{E}_{s_t \sim D, a_t \sim \pi} \left[ Q(s_t, a_t) - \alpha \log \pi(a_t|s_t) \right],
\end{equation}
где 
$\mathbb{E}_{s_t \sim D, a_t \sim \pi}$ -- математическое ожидание, где состояние $s_t$ в момент времени $t$ берется из буфера воспроизведения $D$,
а действие $a_t$ в момент времени берется из политики $\pi$,
$Q(s_t, a_t)$ — функция ценности действия оценивающая ожидаемую совокупную награду при совершении действия, 
$\alpha$ — гиперпараметр, определяющий влияние энтропии, 
$\log \pi(a_t|s_t)$ — энтропия политики.

\section{Deep Deterministic Policy Gradient (DDPG)}
Deep Deterministic Policy Gradient (DDPG) — это метод обучения с подкреплением, 
использующий детерминированную политику и обучающий ее с помощью градиентного спуска \cite{lib:rlmethods}. 
Целевая функция для обновления политики записывается следующим образом:
\begin{equation}
L_{DDPG}(\theta) = \mathbb{E}_{s_t \sim D} \left[ \nabla_\theta \pi_\theta(s_t) \nabla_a Q(s_t, a)|_{a=\pi_\theta(s_t)} \right],
\end{equation}
где 
$\mathbb{E}_{s_t \sim D}$ -- математическое ожидание, где состояние $s_t$ берется из буфера воспроизведения $D$,
$\pi_\theta(s_t)$ -- детерминированная политика агента,
$\nabla_\theta \pi_\theta(s_t)$ -- градиент политики по ее параметрам $\theta$ в состоянии $s_t$,
$Q(s_t, a)$ — функция ценности действия, оценивающая ожидаемую совокупную награду,
$\nabla_a Q(s_t, a)|_{a=\pi_\theta(s_t)}$ -- градиент $Q$-функции, вычисленный в точке $a=\pi_\theta(s_t)$.

Все выражение представляет собой цепочку градиентов, вычисленное по правилу цепочки (chain rule). 
Оно показывает, как нужно изменить параметры политики $\theta$,
чтобы увеличить значение Q-функции, то есть, чтобы политика выдавала действия, ведущие к большей награде.

\section{Twin Delayed Deep Deterministic Policy Gradient (TD3)}
Twin Delayed Deep Deterministic Policy Gradient (TD3) — это улучшение метода DDPG, 
направленное на устранение переоценки функции ценности \cite{lib:rlmethods}. 
Основные усовершенствования включают использование двух критиков и добавление шума для уменьшения переоценок. 
Целевая функция TD3 записывается следующим образом:
\begin{equation}
L_{TD3}(\theta_i) = \mathbb{E}{s_t, a_t, r_t, s{t+1} \sim D} \left[ \left( y_t - Q_{\theta_i}(s_t, a_t) \right)^2 \right] \quad \text{for } i = 1, 2,
\end{equation}
где 
$\mathbb{E}{s_t, a_t, r_t, s{t+1} \sim D}$ -- математическое ожидание по переходам ${s_t, a_t, r_t, s{t+1}}$ из буфера воспроизведения $D$,
${s_t, a_t, r_t, s{t+1}}$ -- соответственно состояние, действие, награда и следующее состояние,
$Q_{\theta_i}(s_t, a_t)$ -- функция ценности действия для $i$-го критика для состояния $s_t$ и действия $a_t$,
$y_t$ -- целевое значение.

Целевое значение вычисляется слудющим образом:
\begin{equation}
    y_t = r_t + \gamma \min_{i=1,2} Q_{\theta_i'}(s_{t+1}, \pi_{\phi'}(s_{t+1}) + \epsilon),
\end{equation}
где
$r_t$ - полученная награда,
$\gamma$ - коэффициент дисконтирования (обычно 0.95-0.99),
$Q_{\theta_i'}(s_{t+1}, a)$ - целевые Q-функции (target Q-networks) с "замороженными" параметрами $\theta_i'$,
$\pi_{\phi'}(s_{t+1})$ - целевая политика,
$\epsilon$ - шум, добавляемый к действию целевой политики для сглаживания,
$\min_{i=1,2} Q_{\theta_i'}(s_{t+1}, ...)$ - минимум из значений двух целевых Q-функций, что и является основным нововведением TD3 для борьбы с переоценкой.

\section*{Вывод}
В данной аналитической части были рассмотрены основные алгоритмы обучения с подкреплением, их особенности и области применения. 
Учитывая сложность задачи управления роботизированной рукой, а также требования к стабильности, 
универсальности и простоте реализации, PPO представляется наиболее подходящим выбором на начальном этапе исследования. 

\clearpage

