\chapter{Конструкторская часть}

\section{Процесс обучения}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{C:/MGTU/BMSTU/3sem/AI/lr12/bag/report/images/reward\_2.png}
    \caption{Цикл обучения с подкреплением}
\end{figure}
Процесс обучения начинается с получения информации о состоянии $s_t$ из среды. 
Основываясь на этом состоянии, агент выполняет дейсвтие $a_t$, тем самым среда перемещается в новое состояние $s_{t+1}$.
Среда даёт вознаграждение агенту $r_{t+1}$. Агент использует вознаграждение как сигнал для корректировки действий,
стремясь максимизировать награду.

\section{Описание среды AdroitHandHammer-v1}
AdroitHandHammer-v1 — это среда для обучения с подкреплением, являющаяся частью библиотеки \texttt{gymnasium\_robotics} и основанная на симуляторе MuJoCo \cite{lib:gymnasium_robotics}.

Среда предоставляет возможности:

\begin{itemize}
    \item наблюдения за:
        \begin{itemize}
        \item положением и ориентацией суставов роботизированной руки, задаваемыми через 24 степени свободы, 
        каждая из которых представлена тремя значениями (синус, косинус, угловая скорость), что в сумме дает 72 вещественных значения;
        \item положением молотка в трёхмерном пространстве, определяемым 3 вещественными значениями (x, y, z);
        \item положением гвоздя в трёхмерном пространстве, определяемым 3 вещественными значениями (x, y, z);
        \item линейной и угловой скоростью молотка, определяемыми 3 вещественными значениями (x, y, z) для каждой компоненты;
        \item расстоянием от головки молотка до шляпки гвоздя, определяемым как вещественная разность координат (x, y, z);
        \item нормализованным вектором, указывающим направление от головки молотка до шляпки гвоздя, определяемым вещественными значениями (x, y, z) в интервале от -1 до 1;
        \item состоянием гвоздя относительно доски, которое задаётся 2 вещественными значениями от 0 (гвоздь не забит) до 1 (гвоздь полностью забит);
        \end{itemize}
    \item управлением 24 степенями свободы роботизированной руки, что позволяет задавать крутящий момент для каждого сустава;
    \item регулированием вознаграждением:
    \begin{itemize}
        \item агент получает награду только если гвоздь полностью забит (разряженная награда);
        \item агент получает награду, основанную на расстоянии между головкой молотка и шляпкой гвоздя, а также на прогрессе забивания гвоздя (плотная награда);
    \end{itemize}
    \item установлением условия завершения: эпизод завершается, если гвоздь полностью забит или по истечении максимального количества шагов;
    \item установкой начального состояния, при котором роботизированная рука находится в исходном положении, молоток лежит на столе, а гвоздь частично вбит в доску;
    \item сбором данных, полученных с помощью различных политик (в том числе экспертных и случайных).
\end{itemize}

\section{Общий ход алгоритма}

Процесс обучения модели машинного обучения можно разделить на несколько основных этапов: 
инициализацию, тренировку, тестирование и визуализацию результатов. 

На этапе инициализации подготавливается все необходимое для обучения и тестирования: 
среда, модель, инструменты для отслеживания процесса обучения.

Тренировка (обучение) является основным этапом, на котором модель подстраивает свои параметры для лучшего выполнения задачи. 
Сначала запускается процесс обучения модели на определенное количество шагов. 
Во время обучения модель взаимодействует со средой следующим образом: она получает данные о текущем состоянии среды (наблюдение), 
выбирает действие на основе своей текущей стратегии, среда изменяет свое состояние в соответствии с действием и выдает награду, 
а модель обновляет свою стратегию, стремясь максимизировать суммарную награду. 
По завершении обучения модель сохраняется для дальнейшего использования.

На этапе тестирования обученная модель проверяется на способность решать поставленную задачу. 
Сначала среда возвращается в начальное состояние. 
Затем модель предсказывает действие на основе текущего состояния, среда выполняет шаг, 
используя предсказанное действие, и возвращает новое состояние, награду и информацию о завершении эпизода. 
Цикл взаимодействия со средой повторяется до завершения эпизода.

Наконец, происходит визуализация результатов. 
Строится и сохраняется график, отображающий изменение показателей в процессе обучения (например, сумму наград). 
Это позволяет визуально оценить эффективность обучения.

\section*{Вывод}

В данной главе были рассмотрены ключевые аспекты процесса обучения с подкреплением на примере среды AdroitHandHammer-v1. 
Описан цикл взаимодействия агента со средой, включающий получение наблюдений, выполнение действий и получение вознаграждений. 
Подробно описана среда AdroitHandHammer-v1, включая ее возможности, структуру наблюдений, действий, наград, а также условия завершения эпизодов и начальное состояние. 
Также был изложен общий алгоритм процесса обучения модели, состоящий из этапов инициализации, тренировки, тестирования и визуализации. 
Таким образом, была подготовлена база для дальнейшего построения и обучения модели, способной успешно решать задачу забивания гвоздя в среде AdroitHandHammer-v1.

\clearpage
